---
title: "Project 2"
author: "Coco Donovan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Loading Packages

```{r}
library(readr)
library(dplyr)
library(mice)
```

### Loading Data

```{r}
tb1 <- read_excel("Copy of Data Columns, Types.xlsx")
tb2 = read_csv('StudentData - TO MODEL.csv')

colnames(tb2) = gsub(" ", "_", colnames(tb2))

tb3 = read_csv('StudentEvaluation- TO PREDICT.csv')

colnames(tb3) = gsub(" ", "_", colnames(tb3))
```

For whatever reason, "StudentData - TO MODEL.xlsx" was being read incorrectly, so I exported the file to csv and read it that way. I chose to read "StudentEvaluation- TOPREDICT.xslx" as a csv for good measure. I did not see a need to export and reread the info file as that file was not impacted. There were ways to specify that certain columns should be read in specific ways; however, the ways in which I tried to specify the columns did not show any improvements.

After running through this process, I realized that NAs were imputed with zeroes, which was not ideal as some there were already zeroes as non-NA values in at least some of the columns. I manually uploaded the data to google sheets and exported through google sheets. This manual process seemed to solve for data quality issues.

### Data Exploration

```{r}
sapply(tb2, is.character)
```

It seems like Brand Code is the only non-numeric value. If the goal is to impute missing values, we could impute missing values using knn (for brand code). I also want to look at what the size of the resulting data set will be if I just apply a remove NA logic.

```{r}
print(nrow(na.omit(tb2))/nrow(tb2) * 100)
```

Removing all NA values leaves us with about 79% of the original data set, which is not so bad until you consider that the prediction data set may have missing NA values and if we end up choosing a variable in the modeling phase with missing NAs in the prediction set, that sets us up for a level of inconsistency. It might just make sense to impute missing values and to do this I will use the mice() function from the mice package. Though before I do this, I want to understand the nature of missingness and the nature of my variables with missing values (most variables). I'll start by checking the variables individually as there might be a reason to exclude an individual variable due to a large amount of missingness.

```{r}
# Calculate percentage of missing values for each variable
percent_missing <- colMeans(is.na(tb2)) * 100

# Create a dataframe to display results
missing_summary <- data.frame(
  variable = names(percent_missing),
  percent_missing = percent_missing
)

print(missing_summary %>% select(percent_missing) %>% arrange(desc(percent_missing)) %>% head())
```

It does not seem that any individual variable has a significant amount of missingness. The variable with the largest amount of missingness, by far, is MFR and is only missing about 8% of its data.

Now, I want to check the distributions of my variables (to assess for normality).

```{r}
numeric_cols <- sapply(tb2, is.numeric)
df_numeric <- tb2[, numeric_cols]

for (col in names(df_numeric)) {
  qqnorm(df_numeric[[col]], main = paste("QQ Plot of", col))
  qqline(df_numeric[[col]], col = 2)  # Adds a line to the QQ plot
}
```

Based on the qqplots, it seems that some variables would benefit from imputation that follows a normal regression and some would be best served by a pmm imputation.

### Imputing Data

```{r}
# Assuming tb2 and tb3 are your original datasets
# Convert factor column to factor in tb2 and tb3 (if necessary)
tb2$Brand_Code <- as.factor(tb2$Brand_Code)
tb3$Brand_Code <- as.factor(tb3$Brand_Code)

# Add dataset indicator column
tb2$dataset <- "train"
tb3$dataset <- "test"

# Combine datasets
combined_data <- rbind(tb2, tb3)

# Define columns for norm imputation and others for pmm
norm_reg_cols <- c('Carb_Volume', 'Fill_Ounces', 'PC_Volume', 'Carb_Pressure', 'Carb_Temp', 'Carb_Pressure1', 'PH')
pmm_cols <- setdiff(names(combined_data), norm_reg_cols)

# Specify blocks for imputation
blocks <- list(
  norm = norm_reg_cols,
  pmm = pmm_cols
)

# Perform mice imputation on combined_data
imputed_data <- mice(data = combined_data, m = 1, method = c('norm', 'pmm'), blocks = blocks, seed = 500)

# Impute and split data back into tb2 (train) and tb3 (test)
df_train <- complete(imputed_data, action = "long", include = FALSE)[combined_data$dataset == "train", ]
df_test <- complete(imputed_data, action = "long", include = FALSE)[combined_data$dataset == "test", ]

df_train <- df_train %>%
  select(-dataset)

df_test <- df_test %>%
  select(-dataset)
```

I combined the two datasets for imputation purposes, though I did specify which rows corresponded to which dataset, so that I could easily then separate the data after imputation. I then employed normal regression or pmm depending on the my assumptions of normality for a given variable based on a variable's qqplot.
